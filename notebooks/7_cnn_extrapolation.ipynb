{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6168cc7b-9f1f-4752-b804-02c334a44ea2",
   "metadata": {},
   "source": [
    "# CNN Extrapolation to 6-Qubit Circuits\n",
    " \n",
    "This notebook evaluates the ability of the CNN model trained on 5 qubit quantum circuits to generalize to unseen 6-qubit circuits. The test includes both:\n",
    "\n",
    "- **Zero-shot extrapolation**: direct evaluation without additional training.\n",
    "- **Few-shot fine-tuning**: limited adaptation using a small subset of 6-qubit circuits.\n",
    "\n",
    "Datasets include Class A (variational) and Class B (QAOA-like) circuits under both noiseless and noisy conditions. Performance is evaluated using KL divergence, classical fidelity, mean squared error (MSE), and Wasserstein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "791811e6-785e-4b38-97c3-51ab6d740afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b6aecf0-ede4-475d-a9ce-61c4e3de9b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import wasserstein_distance\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b315910-d81a-4e20-a4c7-24168f5d6ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility and device setup\n",
    "def set_all_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_all_seeds(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2c542-28be-49c3-bff0-0d07bc2ab25f",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc9241c7-bb5f-4747-8ac2-f6c4e2b68984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_vec(p, q, eps=1e-8):\n",
    "    \"\"\"Computes element-wise KL divergence between two probability distributions p and q.\"\"\"\n",
    "    p = p + eps\n",
    "    q = q + eps\n",
    "    return (p * (p.log() - q.log())).sum(dim=1)\n",
    "\n",
    "\n",
    "def classical_fidelity_vec(p, q, eps=1e-8):\n",
    "    \"\"\"Computes classical fidelity between two distributions as the inner product of square roots.\"\"\"\n",
    "    p = p + eps\n",
    "    q = q + eps\n",
    "    return (p.sqrt() * q.sqrt()).sum(dim=1)\n",
    "\n",
    "\n",
    "def wasserstein_vec(p, q):\n",
    "    \"\"\"Computes Wasserstein-1 distance for each pair of distributions in a batch, skipping invalid samples.\"\"\"\n",
    "    x = np.arange(p.shape[1])\n",
    "    p_np = p.detach().cpu().numpy()\n",
    "    q_np = q.detach().cpu().numpy()\n",
    "    results = []\n",
    "    for i in range(p_np.shape[0]):\n",
    "        if np.sum(p_np[i]) > 0 and np.sum(q_np[i]) > 0:\n",
    "            try:\n",
    "                d = wasserstein_distance(x, x, p_np[i], q_np[i])\n",
    "                if np.isfinite(d):\n",
    "                    results.append(d)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return np.array(results) if len(results) > 0 else np.array([np.nan])\n",
    "\n",
    "\n",
    "def mse_vec(p, q):\n",
    "    \"\"\"Computes mean squared error between two batches of vectors with shape correction.\"\"\"\n",
    "    if p.shape != q.shape:\n",
    "        if p.ndim == 2 and p.shape[0] == 1 and q.ndim == 1:\n",
    "            q = q.unsqueeze(0)\n",
    "        elif q.ndim == 2 and q.shape[0] == 1 and p.ndim == 1:\n",
    "            p = p.unsqueeze(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Shape mismatch in mse_vec: p {p.shape}, q {q.shape}\")\n",
    "    return ((p - q) ** 2).mean(dim=1)\n",
    "\n",
    "\n",
    "def normalize_distribution(tensor, dim=1, eps=1e-8):\n",
    "    \"\"\"Normalizes a tensor along the specified dimension to form a probability distribution.\"\"\"\n",
    "    if tensor.dim() == 1:\n",
    "        return tensor / (tensor.sum() + eps)\n",
    "    return tensor / (tensor.sum(dim=dim, keepdim=True) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216ac4c-b5e5-4a02-be45-2dcc152a4e8e",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd60696-24fe-4626-8aec-3acca248b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_dataset_6q(noise_type, circuit_class):\n",
    "    \"\"\"\n",
    "    Loads a single 6-qubit dataset (class + noise group).\n",
    "    \"\"\"\n",
    "    fname = f\"dataset_6q_{noise_type}_{circuit_class}.pt\"\n",
    "    fpath = os.path.join(\"../datasets/6-qubit\", noise_type, circuit_class, fname)\n",
    "    if not os.path.exists(fpath):\n",
    "        raise FileNotFoundError(f\"Missing 6q dataset: {fpath}\")\n",
    "    data_list = torch.load(fpath)\n",
    "    for g in data_list:\n",
    "        g.circuit_class = circuit_class\n",
    "        g.noise_regime = noise_type\n",
    "        g.n_qubits = 6\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def load_6q_all_groups():\n",
    "    \"\"\"\n",
    "    Loads all 4 groups (A/B Ã— noisy/noiseless) of 6-qubit circuit datasets.\n",
    "    Returns a combined list.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    for cls in [\"classA\", \"classB\"]:\n",
    "        for noise in [\"noiseless\", \"noisy\"]:\n",
    "            group = load_single_dataset_6q(noise, cls)\n",
    "            all_data.extend(group)\n",
    "    random.shuffle(all_data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65689054-f7df-4507-b1e8-a998d7fa77e4",
   "metadata": {},
   "source": [
    "## Sequence Conversion for CNN Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "223e14ef-b87b-4a42-a13e-a04018b7aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_graph_to_sequence(graph_data, max_len=40):\n",
    "    \"\"\"\n",
    "    Converts a graph-based quantum circuit into a fixed-length sequence of gate feature vectors.\n",
    "\n",
    "    Each gate in the circuit is represented by a feature vector (from graph_data.x).\n",
    "    \"\"\"\n",
    "    x = graph_data.x\n",
    "    n_gates, feat_dim = x.shape\n",
    "    pad_len = max(0, max_len - n_gates)\n",
    "    if pad_len > 0:\n",
    "        padding = torch.zeros((pad_len, feat_dim), dtype=x.dtype)\n",
    "        x_padded = torch.cat([x, padding], dim=0)\n",
    "    else:\n",
    "        x_padded = x[:max_len]\n",
    "    return x_padded, graph_data.u, graph_data.y\n",
    "\n",
    "\n",
    "class SequenceCircuitDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that transforms a list of quantum circuit graphs into sequences suitable for 1D CNN input.\n",
    "\n",
    "    Each circuit is represented as a padded sequence of gate-level feature vectors,\n",
    "    along with global features and target probability distributions.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list, max_len=40):\n",
    "        self.data_list = data_list\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        g = self.data_list[idx]\n",
    "        sequence, global_feat, target = convert_graph_to_sequence(g, self.max_len)\n",
    "        return {\n",
    "            'sequence': sequence,\n",
    "            'global': global_feat,\n",
    "            'target': target,\n",
    "            'circuit_class': getattr(g, 'circuit_class', 'unknown'),\n",
    "            'noise_regime': getattr(g, 'noise_regime', 'unknown'),\n",
    "            'n_qubits': g.n_qubits\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_sequence(batch):\n",
    "    \"\"\"\n",
    "    Collates a batch of sequence-encoded circuit samples into batched tensors.\n",
    "    \"\"\"\n",
    "    sequences = torch.stack([item['sequence'] for item in batch])\n",
    "    global_feats = torch.stack([item['global'] for item in batch])\n",
    "    targets = torch.stack([item['target'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        'sequence': sequences,\n",
    "        'global': global_feats,\n",
    "        'target': targets\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabb733-357c-452f-b1c7-d5fc362e6879",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133b1d5f-7062-4420-8af7-1b0e5204bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN model for predicting output distributions of quantum circuits.\n",
    "    Applies a 1D convolution over gate-level sequences, combines with global features,\n",
    "    and passes through a shallow MLP head.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, global_dim, output_dim, hidden_2q=58, n_qubits=2):\n",
    "        super().__init__()\n",
    "        scale = (n_qubits - 1) * 1.3\n",
    "        hidden = int(hidden_2q * scale)\n",
    "\n",
    "        self.conv = nn.Conv1d(input_dim, hidden, kernel_size=5, padding=2)\n",
    "        self.global_proj = nn.Linear(global_dim, hidden)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden, 4 * hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden, 2 * hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, sequence_data, global_features):\n",
    "        x = sequence_data.permute(0, 2, 1)\n",
    "        x = self.conv(x).mean(dim=2)\n",
    "        u_proj = self.global_proj(global_features)\n",
    "        return self.mlp(torch.cat([x, u_proj], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4f6e78-fab3-445f-81b1-d6fbdc4d768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_for_6q(sample, hidden_dim_2q=64):\n",
    "    \"\"\"\n",
    "    Builds CNN model with 6q-compatible output size and scaled hidden size.\n",
    "    \"\"\"\n",
    "    input_dim = sample.x.shape[1]\n",
    "    global_dim = sample.u.shape[0]\n",
    "    output_dim = 64\n",
    "    hidden_dim = int(hidden_dim_2q * (1 + 0.75 * (5 - 2)))\n",
    "\n",
    "    return SequenceCNN(input_dim=input_dim,\n",
    "                       output_dim=output_dim,\n",
    "                       global_dim=global_dim)\n",
    "\n",
    "\n",
    "def load_cnn_weights(model, path):\n",
    "    \"\"\"\n",
    "    Loads CNN model weights from 5q model, interpolating if needed.\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(path, map_location='cpu')\n",
    "    model_state = model.state_dict()\n",
    "    for k in model_state:\n",
    "        if k in state_dict and model_state[k].shape == state_dict[k].shape:\n",
    "            model_state[k] = state_dict[k]\n",
    "    model.load_state_dict(model_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77dc652-971c-4c5c-b926-4c569498544d",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e915cf5-0ed1-4b46-966a-3c5f3ef3ba5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 samples.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceCNN(\n",
       "  (conv): Conv1d(30, 75, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (global_proj): Linear(in_features=9, out_features=75, bias=True)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=150, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=150, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and transform 6-qubit dataset\n",
    "data_6q = load_6q_all_groups()\n",
    "print(f\"Loaded {len(data_6q)} samples.\")\n",
    "cnn_dataset = SequenceCircuitDataset(data_6q)\n",
    "cnn_loader = DataLoader(cnn_dataset, batch_size=32, shuffle=False, collate_fn=collate_sequence)\n",
    "\n",
    "# Build and load CNN model\n",
    "sample_6q = data_6q[0]\n",
    "cnn_model = build_cnn_for_6q(sample_6q)\n",
    "cnn_model = load_cnn_weights(cnn_model, \"../models/cnn_models/5q_cnn.pt\")\n",
    "cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f32c132-bf2d-4588-903d-139a59d80597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot evaluation\n",
      "MSE: 0.000864\n",
      "KL: 0.891041\n",
      "Fidelity: 0.758899\n",
      "Wasserstein: 7.008146\n"
     ]
    }
   ],
   "source": [
    "def evaluate_cnn_zero_shot(model, loader):\n",
    "    model.eval()\n",
    "    all_mse, all_kl, all_fi, all_wass = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            pred = model(batch['sequence'], batch['global'])\n",
    "            pred_prob = F.softmax(pred, dim=1)\n",
    "            target_prob = normalize_distribution(batch['target'])\n",
    "\n",
    "            if torch.isnan(pred_prob).any() or torch.isnan(target_prob).any():\n",
    "                continue\n",
    "\n",
    "            all_mse.append(mse_vec(pred_prob, target_prob).cpu().numpy())\n",
    "            all_kl.append(kl_divergence_vec(target_prob, pred_prob).cpu().numpy())\n",
    "            all_fi.append(classical_fidelity_vec(target_prob, pred_prob).cpu().numpy())\n",
    "            all_wass.append(wasserstein_vec(pred_prob, target_prob))\n",
    "\n",
    "    return {\n",
    "        \"MSE\": float(np.mean(np.concatenate(all_mse))) if all_mse else np.nan,\n",
    "        \"KL\": float(np.mean(np.concatenate(all_kl))) if all_kl else np.nan,\n",
    "        \"Fidelity\": float(np.mean(np.concatenate(all_fi))) if all_fi else np.nan,\n",
    "        \"Wasserstein\": float(np.mean(np.concatenate(all_wass))) if all_wass else np.nan,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run zero-shot\n",
    "print(\"Zero-shot evaluation\")\n",
    "zero_metrics = evaluate_cnn_zero_shot(cnn_model, cnn_loader)\n",
    "for k, v in zero_metrics.items():\n",
    "    print(f\"{k}: {v:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7615de-6135-41ee-b160-3b8ca2596d97",
   "metadata": {},
   "source": [
    "## Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1105442-079a-4d6b-b0dd-ac7d597a9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_few_shot_split(data_list, n_per_group=50):\n",
    "    \"\"\"\n",
    "    Creates a stratified few-shot train/val split across circuit class and noise regime.\n",
    "    \"\"\"\n",
    "    group_buckets = defaultdict(list)\n",
    "    for g in data_list:\n",
    "        key = (getattr(g, 'noise_regime', 'unknown'), getattr(g, 'circuit_class', 'unknown'))\n",
    "        group_buckets[key].append(g)\n",
    "\n",
    "    train_set, val_set = [], []\n",
    "    for key, items in group_buckets.items():\n",
    "        random.shuffle(items)\n",
    "        train_set.extend(items[:n_per_group])\n",
    "        val_set.extend(items[n_per_group:])\n",
    "\n",
    "    return train_set, val_set\n",
    "\n",
    "\n",
    "few_shot_train_graphs, few_shot_val_graphs = stratified_few_shot_split(data_6q, n_per_group=50)\n",
    "\n",
    "train_dataset = SequenceCircuitDataset(few_shot_train_graphs)\n",
    "val_dataset = SequenceCircuitDataset(few_shot_val_graphs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_sequence)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbef7547-97f2-40d1-8a6c-126b6dabe106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(pred_prob, target_prob, eps=1e-8):\n",
    "    \"\"\"Computes mean KL divergence loss between predicted and target probability distributions.\"\"\"\n",
    "    pred_prob = pred_prob + eps\n",
    "    target_prob = target_prob + eps\n",
    "    return torch.mean(torch.sum(target_prob * (target_prob.log() - pred_prob.log()), dim=1))\n",
    "\n",
    "\n",
    "def finetune_cnn(model, train_loader, val_loader, epochs=20, lr=1e-4):\n",
    "    \"\"\"Performs few-shot fine-tuning of the CNN using KL divergence loss.\"\"\"\n",
    "    model.train()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            seq = batch['sequence'].to(device)\n",
    "            glob = batch['global'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "\n",
    "            pred = model(seq, glob)\n",
    "            pred_prob = F.softmax(pred, dim=1)\n",
    "            target_prob = normalize_distribution(target)\n",
    "\n",
    "            loss = kl_loss(pred_prob, target_prob)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                seq = batch['sequence'].to(device)\n",
    "                glob = batch['global'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "\n",
    "                pred = model(seq, glob)\n",
    "                pred_prob = F.softmax(pred, dim=1)\n",
    "                target_prob = normalize_distribution(target)\n",
    "\n",
    "                val_loss = kl_loss(pred_prob, target_prob)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train KL Loss: {np.mean(train_losses):.6f} | Val KL Loss: {np.mean(val_losses):.6f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "338523f2-8016-447b-8f87-8c1348fe0826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train KL Loss: 0.876760 | Val KL Loss: 0.881790\n",
      "Epoch 2 | Train KL Loss: 0.943856 | Val KL Loss: 0.880293\n",
      "Epoch 3 | Train KL Loss: 0.865485 | Val KL Loss: 0.878945\n",
      "Epoch 4 | Train KL Loss: 0.927782 | Val KL Loss: 0.877659\n",
      "Epoch 5 | Train KL Loss: 0.884384 | Val KL Loss: 0.876356\n",
      "Epoch 6 | Train KL Loss: 0.856771 | Val KL Loss: 0.874962\n",
      "Epoch 7 | Train KL Loss: 0.870564 | Val KL Loss: 0.873521\n",
      "Epoch 8 | Train KL Loss: 0.935998 | Val KL Loss: 0.871899\n",
      "Epoch 9 | Train KL Loss: 0.859464 | Val KL Loss: 0.870308\n",
      "Epoch 10 | Train KL Loss: 0.879475 | Val KL Loss: 0.868730\n",
      "Epoch 11 | Train KL Loss: 0.845983 | Val KL Loss: 0.867163\n",
      "Epoch 12 | Train KL Loss: 0.872125 | Val KL Loss: 0.865835\n",
      "Epoch 13 | Train KL Loss: 0.879678 | Val KL Loss: 0.864694\n",
      "Epoch 14 | Train KL Loss: 0.841939 | Val KL Loss: 0.863759\n",
      "Epoch 15 | Train KL Loss: 0.883417 | Val KL Loss: 0.862919\n",
      "Epoch 16 | Train KL Loss: 0.863109 | Val KL Loss: 0.862319\n",
      "Epoch 17 | Train KL Loss: 0.868518 | Val KL Loss: 0.862033\n",
      "Epoch 18 | Train KL Loss: 0.840438 | Val KL Loss: 0.861920\n",
      "Epoch 19 | Train KL Loss: 0.899637 | Val KL Loss: 0.861492\n",
      "Epoch 20 | Train KL Loss: 0.847252 | Val KL Loss: 0.860797\n",
      "Few-shot Evaluation Metrics:\n",
      "MSE: 0.000849\n",
      "KL: 0.867918\n",
      "Fidelity: 0.760577\n",
      "Wasserstein: 7.096840\n"
     ]
    }
   ],
   "source": [
    "model_finetuned = finetune_cnn(cnn_model, train_loader, val_loader)\n",
    "\n",
    "model_finetuned.eval()\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        seq = batch['sequence'].to(device)\n",
    "        glob = batch['global'].to(device)\n",
    "        target = batch['target'].to(device)\n",
    "\n",
    "        pred = model_finetuned(seq, glob)\n",
    "        pred_prob = F.softmax(pred, dim=1)\n",
    "        target_prob = normalize_distribution(target)\n",
    "\n",
    "        all_preds.append(pred_prob)\n",
    "        all_targets.append(target_prob)\n",
    "\n",
    "preds = torch.cat(all_preds, dim=0)\n",
    "targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "few_shot_metrics = {\n",
    "    \"MSE\": float(mse_vec(preds, targets).mean().item()),\n",
    "    \"KL\": float(kl_divergence_vec(targets, preds).mean().item()),\n",
    "    \"Fidelity\": float(classical_fidelity_vec(targets, preds).mean().item()),\n",
    "    \"Wasserstein\": float(wasserstein_vec(preds, targets).mean().item())\n",
    "}\n",
    "\n",
    "print(\"Few-shot Evaluation Metrics:\")\n",
    "for k, v in few_shot_metrics.items():\n",
    "    print(f\"{k}: {v:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824098a-e3f1-40d3-9c93-d8dd06fbfe54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
