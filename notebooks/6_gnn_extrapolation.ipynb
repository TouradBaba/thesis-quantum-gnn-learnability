{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a57a7d-7347-497e-86c9-091bd73b3437",
   "metadata": {},
   "source": [
    "# GNN Extrapolation to 6-Qubit Circuits\n",
    "\n",
    "This notebook evaluates the ability of the GNN model trained on 5 qubit quantum circuits to generalize to unseen 6-qubit circuits. The test includes both:\n",
    "\n",
    "- **Zero-shot extrapolation**: direct evaluation without additional training.\n",
    "- **Few-shot fine-tuning**: limited adaptation using a small subset of 6-qubit circuits.\n",
    "\n",
    "Datasets include Class A (variational) and Class B (QAOA-like) circuits under both noiseless and noisy conditions. Performance is evaluated using KL divergence, classical fidelity, mean squared error (MSE), and Wasserstein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cafb357-90c3-4666-90de-0424ae30f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da0ead5-6351-4844-9464-05cc60ebae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Linear, Sequential, ReLU, Dropout, BatchNorm1d, LayerNorm\n",
    "from torch_geometric.nn import TransformerConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "from scipy.stats import wasserstein_distance\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1986a-c32a-401e-a5c8-7e78f2ff2b9b",
   "metadata": {},
   "source": [
    "## Seeding and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05eb5fb0-c7d2-4d74-896f-f6a29acefa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "def set_all_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_all_seeds(42)\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d2327-264f-4100-9a48-efb041e55034",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06068a4e-4426-44ec-b710-d0d8bf20b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_vec(p, q, eps=1e-8):\n",
    "    \"\"\"Computes element-wise KL divergence between two probability distributions p and q.\"\"\"\n",
    "    p = p + eps\n",
    "    q = q + eps\n",
    "    return (p * (p.log() - q.log())).sum(dim=1)\n",
    "\n",
    "\n",
    "def classical_fidelity_vec(p, q, eps=1e-8):\n",
    "    \"\"\"Computes classical fidelity between two distributions as the inner product of square roots.\"\"\"\n",
    "    p = p + eps\n",
    "    q = q + eps\n",
    "    return (p.sqrt() * q.sqrt()).sum(dim=1)\n",
    "\n",
    "\n",
    "def wasserstein_vec(p, q):\n",
    "    \"\"\"Computes Wasserstein-1 distance for each pair of distributions in a batch, skipping invalid samples.\"\"\"\n",
    "    x = np.arange(p.shape[1])\n",
    "    p_np = p.detach().cpu().numpy()\n",
    "    q_np = q.detach().cpu().numpy()\n",
    "    results = []\n",
    "    for i in range(p_np.shape[0]):\n",
    "        if np.sum(p_np[i]) > 0 and np.sum(q_np[i]) > 0:\n",
    "            try:\n",
    "                d = wasserstein_distance(x, x, p_np[i], q_np[i])\n",
    "                if np.isfinite(d):\n",
    "                    results.append(d)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return np.array(results) if len(results) > 0 else np.array([np.nan])\n",
    "\n",
    "\n",
    "def mse_vec(p, q):\n",
    "    \"\"\"Computes mean squared error between two batches of vectors with shape correction.\"\"\"\n",
    "    if p.shape != q.shape:\n",
    "        if p.ndim == 2 and p.shape[0] == 1 and q.ndim == 1:\n",
    "            q = q.unsqueeze(0)\n",
    "        elif q.ndim == 2 and q.shape[0] == 1 and p.ndim == 1:\n",
    "            p = p.unsqueeze(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Shape mismatch in mse_vec: p {p.shape}, q {q.shape}\")\n",
    "    return ((p - q) ** 2).mean(dim=1)\n",
    "\n",
    "\n",
    "def normalize_distribution(tensor, dim=1, eps=1e-8):\n",
    "    \"\"\"Normalizes a tensor along the specified dimension to form a probability distribution.\"\"\"\n",
    "    if tensor.dim() == 1:\n",
    "        return tensor / (tensor.sum() + eps)\n",
    "    return tensor / (tensor.sum(dim=dim, keepdim=True) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111db5d-b9c9-47b7-95f1-02dc6213ca48",
   "metadata": {},
   "source": [
    "## Topological Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc3b88a-8afb-436e-acb8-69e4606d7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_topological_position_feature(data):\n",
    "    \"\"\"\n",
    "    Appends a normalized topological position feature to each node in a DAG-based circuit graph.\n",
    "\n",
    "    The feature encodes each node's relative position in a topological sort of the graph.\n",
    "    \"\"\"\n",
    "    G = to_networkx(data, to_undirected=False)\n",
    "    topo_order = list(nx.topological_sort(G))\n",
    "    pos = torch.zeros((data.num_nodes, 1), dtype=torch.float32)\n",
    "    for i, node_id in enumerate(topo_order):\n",
    "        pos[node_id] = i / (len(topo_order) - 1) if len(topo_order) > 1 else 0.0\n",
    "    data.x = torch.cat([data.x, pos], dim=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c02af4-959f-43a1-899c-a9e2644e4164",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67eff2c8-f345-4660-83cb-fdd4a07384af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_dataset_6q(noise_type, circuit_class):\n",
    "    \"\"\"\n",
    "    Loads a single 6-qubit dataset (class + noise group) and adds metadata + position feature.\n",
    "    \"\"\"\n",
    "    fname = f\"dataset_6q_{noise_type}_{circuit_class}.pt\"\n",
    "    fpath = os.path.join(\"../datasets/6-qubit\", noise_type, circuit_class, fname)\n",
    "    if not os.path.exists(fpath):\n",
    "        raise FileNotFoundError(f\"Missing 6q dataset: {fpath}\")\n",
    "    data_list = torch.load(fpath)\n",
    "    for g in data_list:\n",
    "        g.circuit_class = circuit_class\n",
    "        g.noise_regime = noise_type\n",
    "        g.n_qubits = 6\n",
    "        g = add_topological_position_feature(g)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def load_6q_all_groups():\n",
    "    \"\"\"\n",
    "    Loads all 4 groups (A/B Ã— noisy/noiseless) of 6-qubit circuit graphs.\n",
    "    Returns a single combined list.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    for cls in [\"classA\", \"classB\"]:\n",
    "        for noise in [\"noiseless\", \"noisy\"]:\n",
    "            group = load_single_dataset_6q(noise, cls)\n",
    "            all_data.extend(group)\n",
    "    random.shuffle(all_data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed200a-f2f9-443d-8f04-09e4f0c862b4",
   "metadata": {},
   "source": [
    "## GNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a52402-458f-472a-a11b-a621e225bf43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GraphTransformer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Graph neural network model using TransformerConv layers to predict output distributions of quantum circuits.\n",
    "\n",
    "    Applies multiple TransformerConv blocks with residual connections, followed by global pooling and a deep MLP head.\n",
    "    \"\"\"\n",
    "    def __init__(self, node_in_dim, global_u_dim, edge_attr_dim,\n",
    "                 hidden_dim, output_dim, n_qubits, heads=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = node_in_dim + global_u_dim\n",
    "\n",
    "        self.convs = torch.nn.ModuleList([\n",
    "            TransformerConv(self.input_dim if i == 0 else hidden_dim * heads, hidden_dim, heads=heads,\n",
    "                           dropout=dropout, edge_dim=edge_attr_dim)\n",
    "            for i in range(n_qubits)\n",
    "        ])\n",
    "        self.bns = torch.nn.ModuleList([\n",
    "            BatchNorm1d(hidden_dim * heads)\n",
    "            for _ in range(n_qubits)\n",
    "        ])\n",
    "        self.norm = LayerNorm(hidden_dim * heads)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "        # Widen and deepen MLP\n",
    "        self.mlp = Sequential(\n",
    "            Linear(2 * hidden_dim * heads, 2 * hidden_dim * heads),\n",
    "            ReLU(),\n",
    "            Dropout(dropout),\n",
    "            Linear(2 * hidden_dim * heads, hidden_dim * heads),\n",
    "            ReLU(),\n",
    "            Dropout(dropout),\n",
    "            Linear(hidden_dim * heads, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, u, edge_attr = data.x, data.edge_index, data.batch, data.u, data.edge_attr\n",
    "\n",
    "        if u.dim() == 1:\n",
    "            batch_size = batch.max().item() + 1\n",
    "            u = u.view(batch_size, -1)\n",
    "        u_per_node = u[batch]\n",
    "\n",
    "        x = torch.cat([x, u_per_node], dim=1)\n",
    "        x_in = x\n",
    "        for i in range(len(self.convs)):\n",
    "            x_out = self.dropout(self.relu(self.bns[i](self.convs[i](x_in, edge_index, edge_attr))))\n",
    "            if i > 0:\n",
    "                x_out = x_out + x_in\n",
    "            x_in = x_out\n",
    "        x_final = self.norm(x_in)\n",
    "\n",
    "        x_mean = global_mean_pool(x_final, batch)\n",
    "        x_max = global_max_pool(x_final, batch)\n",
    "        x_pooled = torch.cat([x_mean, x_max], dim=1)\n",
    "\n",
    "        return self.mlp(x_pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a7fe35-5e89-4541-ab9c-d4885aa10921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_5q_model_for_6q_output(sample, hidden_dim_2q=48, heads=3, dropout=0.1):\n",
    "    \"\"\"Builds a 5-qubit GNN model adapted for 6-qubit output size using scaled hidden dimensions.\"\"\"\n",
    "    node_in_dim = sample.x.shape[1]\n",
    "    global_u_dim = sample.u.shape[0]\n",
    "    edge_attr_dim = sample.edge_attr.shape[1]\n",
    "    output_dim = 64\n",
    "    n_qubits = 5\n",
    "\n",
    "    hidden_dim = int(hidden_dim_2q * (1 + 0.75 * (n_qubits - 2)))\n",
    "\n",
    "    return GraphTransformer(\n",
    "        node_in_dim=node_in_dim,\n",
    "        global_u_dim=global_u_dim,\n",
    "        edge_attr_dim=edge_attr_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_dim,\n",
    "        n_qubits=n_qubits,\n",
    "        heads=heads,\n",
    "        dropout=dropout\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4066a9-f1b0-44e5-b066-0ab4f6b8602c",
   "metadata": {},
   "source": [
    "## Load 5 qubits model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4d9272e-dcbc-4068-a742-41cd2f08080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_5q_weights(model, path):\n",
    "    \"\"\"Loads weights from a 5-qubit GNN model.\"\"\"\n",
    "    pretrained = torch.load(path, map_location='cpu')\n",
    "    model_state = model.state_dict()\n",
    "    filtered = {k: v for k, v in pretrained.items() if k in model_state and v.shape == model_state[k].shape}\n",
    "    model_state.update(filtered)\n",
    "    model.load_state_dict(model_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99d3ba-ac69-4207-8299-c93163fcfa47",
   "metadata": {},
   "source": [
    "## Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69323880-a8bf-481d-8395-1f5d17ab0cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_zero_shot(model, data_list):\n",
    "    \"\"\"Performs zero-shot evaluation on 6-qubit circuits using the pretrained 5-qubit model.\"\"\"\n",
    "    model.eval()\n",
    "    loader = DataLoader(data_list, batch_size=32, shuffle=False)\n",
    "    all_mse, all_kl, all_fi, all_wass = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            target = batch.y\n",
    "            target = target.view(pred.shape)\n",
    "            pred_prob = F.softmax(pred, dim=1)\n",
    "            target_prob = normalize_distribution(target)\n",
    "            if torch.isnan(pred_prob).any() or torch.isnan(target_prob).any():\n",
    "                continue\n",
    "            all_mse.append(mse_vec(pred_prob, target_prob).cpu().numpy())\n",
    "            all_kl.append(kl_divergence_vec(target_prob, pred_prob).cpu().numpy())\n",
    "            all_fi.append(classical_fidelity_vec(target_prob, pred_prob).cpu().numpy())\n",
    "            all_wass.append(wasserstein_vec(pred_prob, target_prob))\n",
    "\n",
    "    return {\n",
    "        \"MSE\": float(np.mean(np.concatenate(all_mse))) if all_mse else np.nan,\n",
    "        \"KL\": float(np.mean(np.concatenate(all_kl))) if all_kl else np.nan,\n",
    "        \"Fidelity\": float(np.mean(np.concatenate(all_fi))) if all_fi else np.nan,\n",
    "        \"Wasserstein\": float(np.mean(np.concatenate(all_wass))) if all_wass else np.nan,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20566840-1bf7-4931-bb2c-dbabc975f81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 samples.\n",
      "Evaluation\n",
      "MSE: 0.000866\n",
      "KL: 0.896023\n",
      "Fidelity: 0.757905\n",
      "Wasserstein: 7.031290\n"
     ]
    }
   ],
   "source": [
    "# Load 6-qubit dataset\n",
    "data_6q = load_6q_all_groups()\n",
    "print(f\"Loaded {len(data_6q)} samples.\")\n",
    "sample_6q = data_6q[0]\n",
    "\n",
    "# Build and load model\n",
    "model_6q = build_5q_model_for_6q_output(sample_6q)\n",
    "model_6q = load_5q_weights(model_6q, \"../models/gnn_models/5q_gnn.pt\")\n",
    "model_6q.to(device)\n",
    "\n",
    "# Run zero-shot evaluation\n",
    "print(\"Evaluation\")\n",
    "zero_shot_metrics = evaluate_model_zero_shot(model_6q, data_6q)\n",
    "\n",
    "for k, v in zero_shot_metrics.items():\n",
    "    print(f\"{k}: {v:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262fcb4-e0cc-4c07-a534-0ea5bf0ff748",
   "metadata": {},
   "source": [
    "## Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ba81975-f6d4-44f5-9822-56141fc8c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_few_shot_split(data_list, n_per_group=100):\n",
    "    \"\"\"Creates a stratified few-shot train/val split across circuit class and noise regime.\"\"\"\n",
    "    group_buckets = defaultdict(list)\n",
    "    for g in data_list:\n",
    "        key = (g.noise_regime, g.circuit_class)\n",
    "        group_buckets[key].append(g)\n",
    "    \n",
    "    train_set, val_set = [], []\n",
    "    for key, items in group_buckets.items():\n",
    "        random.shuffle(items)\n",
    "        train_set.extend(items[:n_per_group])\n",
    "        val_set.extend(items[n_per_group:])\n",
    "    \n",
    "    return train_set, val_set\n",
    "\n",
    "\n",
    "few_shot_train, few_shot_val = stratified_few_shot_split(data_6q, n_per_group=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eac7fd13-27ac-4004-81d3-b2829aba7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(pred_prob, target_prob, eps=1e-8):\n",
    "    \"\"\"Computes mean KL divergence loss between predicted and target probability distributions.\"\"\"\n",
    "    pred_prob = pred_prob + eps\n",
    "    target_prob = target_prob + eps\n",
    "    return torch.mean(torch.sum(target_prob * (target_prob.log() - pred_prob.log()), dim=1))\n",
    "\n",
    "\n",
    "def finetune_gnn(model, train_data, val_data, epochs=20, lr=1e-4):\n",
    "    \"\"\"Performs few-shot fine-tuning of the GNN using KL divergence loss.\"\"\"\n",
    "    model.train()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            pred_prob = F.softmax(pred, dim=1)\n",
    "\n",
    "            target = torch.stack([g.y for g in batch.to_data_list()]).to(device)\n",
    "            target_prob = normalize_distribution(target)\n",
    "\n",
    "            loss = kl_loss(pred_prob, target_prob)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # Val KL loss\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                pred = model(batch)\n",
    "                pred_prob = F.softmax(pred, dim=1)\n",
    "\n",
    "                target = torch.stack([g.y for g in batch.to_data_list()]).to(device)\n",
    "                target_prob = normalize_distribution(target)\n",
    "\n",
    "                loss = kl_loss(pred_prob, target_prob)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train KL Loss: {np.mean(train_losses):.6f} | Val KL Loss: {np.mean(val_losses):.6f}\")\n",
    "\n",
    "    #Eval\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred_prob = F.softmax(model(batch), dim=1)\n",
    "\n",
    "            target = torch.stack([g.y for g in batch.to_data_list()]).to(device)\n",
    "            target_prob = normalize_distribution(target)\n",
    "\n",
    "            all_preds.append(pred_prob)\n",
    "            all_targets.append(target_prob)\n",
    "\n",
    "        preds = torch.cat(all_preds, dim=0)\n",
    "        targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    return model, preds, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d043ab5-c5a4-41ac-88da-cc52b988a100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train KL Loss: 0.914996 | Val KL Loss: 0.883256\n",
      "Epoch 2 | Train KL Loss: 0.906543 | Val KL Loss: 0.879700\n",
      "Epoch 3 | Train KL Loss: 0.890341 | Val KL Loss: 0.877100\n",
      "Epoch 4 | Train KL Loss: 0.906791 | Val KL Loss: 0.873458\n",
      "Epoch 5 | Train KL Loss: 0.874387 | Val KL Loss: 0.869623\n",
      "Epoch 6 | Train KL Loss: 0.867210 | Val KL Loss: 0.865018\n",
      "Epoch 7 | Train KL Loss: 0.844165 | Val KL Loss: 0.860535\n",
      "Epoch 8 | Train KL Loss: 0.913162 | Val KL Loss: 0.857326\n",
      "Epoch 9 | Train KL Loss: 0.863229 | Val KL Loss: 0.855532\n",
      "Epoch 10 | Train KL Loss: 0.812205 | Val KL Loss: 0.852619\n",
      "Epoch 11 | Train KL Loss: 0.861643 | Val KL Loss: 0.849727\n",
      "Epoch 12 | Train KL Loss: 0.867958 | Val KL Loss: 0.847630\n",
      "Epoch 13 | Train KL Loss: 0.878863 | Val KL Loss: 0.847009\n",
      "Epoch 14 | Train KL Loss: 0.836234 | Val KL Loss: 0.845868\n",
      "Epoch 15 | Train KL Loss: 0.843282 | Val KL Loss: 0.845993\n",
      "Epoch 16 | Train KL Loss: 0.846929 | Val KL Loss: 0.846275\n",
      "Epoch 17 | Train KL Loss: 0.885862 | Val KL Loss: 0.847277\n",
      "Epoch 18 | Train KL Loss: 0.847155 | Val KL Loss: 0.846389\n",
      "Epoch 19 | Train KL Loss: 0.831762 | Val KL Loss: 0.846203\n",
      "Epoch 20 | Train KL Loss: 0.808956 | Val KL Loss: 0.845393\n",
      "Metrics: {'MSE': 0.0008400144870392978, 'KL': 0.8523653149604797, 'Fidelity': 0.7611618638038635, 'Wasserstein': 7.020495641239846}\n"
     ]
    }
   ],
   "source": [
    "model_finetuned, preds, targets = finetune_gnn(model_6q, few_shot_train, few_shot_val)\n",
    "few_shot_metrics = {\n",
    "    \"MSE\": float(mse_vec(preds, targets).mean().item()),\n",
    "    \"KL\": float(kl_divergence_vec(targets, preds).mean().item()),\n",
    "    \"Fidelity\": float(classical_fidelity_vec(targets, preds).mean().item()),\n",
    "    \"Wasserstein\": float(wasserstein_vec(preds, targets).mean().item())\n",
    "}\n",
    "print(\"Metrics:\", few_shot_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec0421b-d0f6-4c3b-80f7-3ce44afc6257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
